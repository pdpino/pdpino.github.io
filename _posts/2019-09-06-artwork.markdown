---
layout: post
title:  "Artwork recommendations"
date:   2019-09-06 10:00:00 -0400
categories: week4
week: "4"
---

**Paper:** Messina, P., Dominguez, V., Parra, D., Trattner, C., & Soto, A. (2019). Content-based artwork recommendation: integrating painting metadata with neural and manually-engineered visual features. User Modeling and User-Adapted Interaction, 29(2), 251-290.
{: .paper-name}

The paper presents a study on the artwork recommendation problem, which has some differences with other typical recommendation scenarios, the main one being the "one-of-a-kind" characteristic.
Since the physical artwork pieces can be bought and sold just once, you can't relate two (or more) users by which items they both consumed, opposed to movies or music recommendation where you can purchase an item an unlimited amount of times.
Thus, they applied content-based recommendation techniques to solve the task.

In particular, they tested a lot of approaches, multiple ways to calculate features from the images and multiple ways to rank them, including manually curated features, automatically learned ones and hybrid methods.
One thing that I liked, is that they tried to understand and explain the performance differences from each approach, and tried to determine what was the improvement that each model presented.
Besides, they assessed accuracy with many metrics, but also discussed diversity and explainability, which gives a sense of a very thorough study.


Probably the main conclusion to take from the experiments, is that each model has its own advantages, and that an hybrid methods tends to perform the best, as they put together the best from each solution.
A good example of this, is the analysis on the Favorite Artist (FA) method, which performs very well on its own, as users tend to buy or be interested in paintings from the same artist.
Even more, it is not easy to obtain a better performance than this method, which makes it a good simple baseline.
Nonetheless, the FA idea can be used as a _context-aware recommendation_, to help boost some other well-performing methods, such as the DNN approaches shown.
In this scenario, instead of going through the whole dataset to recommend, the search space could be narrowed down to only the known artists and/or the similar ones, which could improve the results.


I would have liked an online evaluation with real users, not just experts (though such a study could be another paper on its own).
The experts may bring insights on how useful are the visual features, and they can check if the recommendations _make sense_, which is very good, but the users would be the ones actually using the system.
It would be nice to see a user study of this, possibly deployed on the actual platform, to determine which system performs better for the users, either capturing data implicitly (such as clicks, purchases, and more), or explicitly, by actually asking the users how good was the recommendation.
